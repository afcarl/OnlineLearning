% $Header: /data/cvsroot/Courses/OnlineLearning/talks/talk2/talk2.tex,v 1.5 2006/01/17 08:11:25 yfreund Exp $

\documentclass{beamer}
%\documentclass[handout]{beamer}
% This file is a solution template for:

% - Giving a talk on some subject.
% - The talk is between 15min and 45min long.
% - Style is ornate.

% Copyright 2004 by Till Tantau <tantau@users.sourceforge.net>.
%
% In principle, this file can be redistributed and/or modified under
% the terms of the GNU Public License, version 2.
%
% However, this file is supposed to be a template to be modified
% for your own needs. For this reason, if you use this file as a
% template and not specifically distribute it as part of a another
% package/program, I grant the extra permission to freely copy and
% modify this file as you see fit and even to delete this copyright
% notice. 


\mode<presentation>
{
  \usetheme{Montpellier}

  %\setbeamercovered{transparent}
  % or whatever (possibly just delete it)
}

\usepackage{xmpmulti} % package that defines \multiinclude

\usepackage[english]{babel}
\usepackage[latin1]{inputenc}

\usepackage{times}
\usepackage[T1]{fontenc}
% Or whatever. Note that the encoding and the font should match. If T1
% does not look nice, try deleting the line with the fontenc.

\title[\ouralg] % (optional, use only with long paper titles)
{The \ouralg Algorithm}

\author[Freund] % (optional, use only with lots of authors)
{Yoav Freund}
% - Give the names in the same order as the appear in the paper.
% - Use the \inst{?} command only if the authors have different
%   affiliation.

\institute[Universities of Somewhere and Elsewhere] % (optional, but mostly needed)

\subject{Machine Learning}
% This is only inserted into the PDF information catalog. Can be left
% out. 

% If you have a file called "university-logo-filename.xxx", where xxx
% is a graphic format that can be processed by latex or pdflatex,
% resp., then you can add a logo as follows:

% \pgfdeclareimage[height=0.5cm]{university-logo}{university-logo-filename}
% \logo{\pgfuseimage{university-logo}}



% Delete this, if you do not want the table of contents to pop up at
% the beginning of each subsection:
%% \AtBeginSubsection[]
%% {
%%   \begin{frame}<beamer>
%%     \frametitle{Outline}
%%     \tableofcontents[currentsection,currentsubsection]
%%   \end{frame}
%% }


% If you wish to uncover everything in a step-wise fashion, uncomment
% the following command: 

\beamerdefaultoverlayspecification{<+->}

\newcommand{\R}[1]{{\color{red}{#1}}}
\newcommand{\B}[1]{{\color{blue}{#1}}}
\newcommand{\W}{\vec{W}}
\newcommand{\V}{\vec{V}}
\newcommand{\X}{\vec{X}}
\newcommand{\loss}{\vec{\ell}}
\newcommand{\HedgeLoss}{L_{\mbox{\footnotesize Hedge}}}

\input{macros}

\begin{document}

%\iffalse %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\fi %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
  \titlepage
\end{frame}

\begin{frame}
  \frametitle{Outline}
  \tableofcontents[pausesections]
  % You might wish to add the option [pausesections]
\end{frame}

\section{\ouralg Algorithm}

\begin{frame}
\frametitle{The hedging problem}

\begin{itemize}
\item \R{$N$} possible actions 

\item At each time step \R{$t=1,2,\ldots,T$}:
\begin{itemize}
\item Algorithm chooses a distribution \R{$\distvec{t}$} over actions.
\item Losses \R{$0 \leq \cost{t}{i} \leq 1$} of all actions \R{$i=1,\ldots,N$} are revealed.
\item Algorithm suffers {\bf expected} loss \R{$\distvec{t} \cdot \costvec{t}$}
\end{itemize}
\item {\bf Goal:} minimize total expected loss
\item {\bf What is \R{$\distvec{t}$}?}
  \begin{itemize}
    \item Distribution wealth in a portfolio.
    \item The algorithm chooses one of the actions at random.
  \end{itemize}
\item Prediction is a {\bf game} played between algorithm and nature,
  in which the goal of the algorithm is to minimize {\bf regret}.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Experts vs. actions}

\begin{itemize}
\item Experts interaction (1 round):
\begin{enumerate}
\item {\bf Experts} make their predictions.
\item {\bf Algorithm} makes it's prediction.
\item {\bf Nature} Chooses label/outcome.
\item {\bf Loss} is associated with each action according to a loss function.
\end{enumerate}

\item Actions interaction (1 round):
\begin{enumerate}
\item {\bf Algorithm} a distribution over the actions.
\item {\bf Nature} reveals the loss of each action.
\item {\bf Loss} of algorithm is expected loss wrt to chosen distribution.
\end{enumerate}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Hedging vs. Halving / pros and cons}
\begin{itemize}
\item {\bf Expert framework assumes more structure:}
  algorithm gets expert predicions before choosing it's own
  prediction. The set of loss vectors is restricted - better for
  algorithm. Experts framework achieves better bounds.
\item {\bf Actions framework assumes bounded losses:} In exprets
  framework, we can deal with some unbounded loss functions.
\item {\bf Partial visibility:} In experts framework, the outcome
  defines {\em all} of the losses. In actions framework you might not
  know the loss of all of the actions (the {\em multiple-arm-bandit
    problem}).
\item {\bf Actions framework is simple and general}: We can usually
  get bounds in the experts framework using algorithm designed for the
  actions framework, but the constants can be worse.
\item {\bf In this class, we will concentrate on the actions
  framework}.
\item {\bf The halving setup works equally for both}
\end{itemize}
\end{frame}

\subsection{Hedging vs. Halving}

\begin{frame}
\frametitle{Hedging vs. Halving}
\begin{itemize}
\item Like halving - we want to zoom into best action.
\item Unlike halving - no action is perfect.
\item Basic idea - reduce probability of lossy actions, \\
but {\color{blue}not all the way to zero}.
\item {\bf Modified Goal:}
minimize {\bf Regret=}{\color{blue}{difference between}} \\
expected total loss \\
{\color{blue}{and}} \\
minimal total loss of repeating one action.
\R{\[
\sum_{t=1}^T \distvec{t} \cdot \costvec{t} - \min_i \left(\sum_{t=1}^T \cost{t}{i} \right)
\]}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Using Hedge to generalize halving alg.}
\begin{itemize}
\item Suppose that there is no perfect action.
\item actions: \R{$i\;\;$} for \R{$i \in (1,2,\ldots,N)$}
\item Now each iteration \R{$t\in\{1,\ldots,T\}$} of game consists of \B{two} steps:
\begin{itemize}
\item Algorithm chooses a distribution \R{$\distvec{t}=(\dist{t}{1},\ldots,\dist{t}{N})$} over the actions.
\item Nature chooses the loss of each action:
  \R{$\costvec{t}=(\ell_1^t,\ldots,\ell_N^t)$}, \R{$\ell_i^t \in [0,1]$}
\end{itemize}
\item Algorithm's cumulative loss is 
\R{$L_A^T=\sum_{t=1}^T \distvec{t} \cdot \costvec{t}$}
\item Cumulative loss of action \R{$i$} is 
\R{$L_i^t = \sum_{t=1}^T \cost{t}{i} $}
\item Our goal is to minimize the {\bf regret:} \R{$L_A^t - \min_i
  L_i^t$} for all \R{$t$} and all possible loss sequences.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{The \ouralg Algorithm}
Consider action \R{$i$} at time \R{$t$}
\begin{itemize}
\item Total loss:
\R{$$L_i^t = \sum_{s=1}^{t-1} \ell_i^s$$}
\item Weight:
\R{$$\wt{t}{i} = e^{-\eta L_i^t}$$}
\item
\R{$\eta>0$} is the learning rate parameter. Halving: \R{$\eta \to \infty$}
\item Probability:
\R{$$\dist{t}{i} = \frac{\wt{t}{i}}{\sum_{j=1}^N \wt{t}{j}},\;\;
\pause     \distvec{t} = \frac{\wtvec{t}}{\sum_{j=1}^N \wt{t}{i}}$$}
\end{itemize}
\end{frame}

\iffalse
\begin{frame} 
\frametitle{Choosing the initial weights} 

\begin{itemize}
\item Giving an action high initial weight makes alg perform well
  \R{if} that action performs well.
\item If good action has low initial weight, our total loss will
  be larger.
\item As \R{$\sum_{i=1}^n \wt{1}{i} = 1$} increasing one weight
  implies decreasing some others.
\item Plays a similar role to prior distribution in Bayesian
  algorithms. 
\end{itemize}

\end{frame} 
\fi

\section{Bound on total loss}
\begin{frame}
\frametitle{Bound on the loss of \ouralg Algorithm}
\begin{itemize}
\item
\begin{theorem}[main theorem] \label{thm:basic-bnd}
For any sequence of loss vectors \R{$\costvec{1},\ldots,\costvec{\iter}$},
and for any \R{$i\in\{1,\ldots,N\}$}, we have
\R{\begin{equation*}
\lossouralg \leq \frac{\ln(N) + \eta \lossi{i}}
		      {1-e^{-\eta}}.
\end{equation*}}
%% More generally, for any nonempty set $S\subseteq\{1,\ldots,N\}$, we have
%% \begin{equation}\label{eqn:set-bnd}
%% \lossouralg \leq \frac{-\ln(\sum_{i\in S}\wt{1}{i})
%% 			 - \eta \max_{i\in S} \lossi{i}}
%% 		      {1-e^{-eta}}.
%% \end{equation}
\end{theorem}
\item
\R{Proof}: by combining upper and lower bounds on \R{$\sumwts{\iter+1}$}
\end{itemize}
\end{frame}

\subsection{Upper bound on $\sumwts{\iter+1}$}

\begin{frame}
\frametitle{Upper bound on \R{$\sumwts{\iter+1}$}}
\begin{lemma}[upper bound] 
For any sequence of loss vectors \R{$\costvec{1},\ldots,\costvec{\iter}$}
we have
\R{\[
\ln\paren{\sumwts{\iter+1}} \leq -(1-e^{-\eta}) \lossouralg.
\]}
\end{lemma}
\end{frame}

\begin{frame}
\frametitle{Proof of upper bound (slide 1)}
\begin{itemize}
\item
If \R{$a \geq 0$} then \R{$a^r$} is convex.
\item For \R{$r\in [0,1]$}, \R{$a^r \leq 1-(1-a)r$}
\item
\includegraphics[height=6cm]{figures/Convexity.pdf}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Proof of upper bound (slide 2)}
Applying \R{$a^r \leq 1-(1-a)^r$} where \R{$a=e^{-\eta}$,$r=\cost{t}{i}$}
\R{
\begin{eqnarray*}
\sumwts{t+1} &= & 
 \sum_{i=1}^N \wt{t}{i} e^{-\eta \cost{t}{i}} \\
~\pause  &\leq&  
 \sum_{i=1}^N \wt{t}{i} \left( 1-(1-e^{-\eta})\cost{t}{i}\right) \\
~\pause  &=& 
 \paren{\sumwts{t}} \paren{ 1-(1-e^{-\eta}) \frac{\wtvec{t}}{\sumwts{t}} \cdot \costvec{t}}\\
~\pause  &=& 
 \paren{\sumwts{t}} \paren{ 1-(1-e^{-\eta}) \distvec{t}\cdot\costvec{t} }
\end{eqnarray*}
}
\end{frame}

\begin{frame}
\frametitle{Proof of upper bound (slide 3)}
\begin{itemize}
\item Combining 
\R{\[
\sumwts{t+1} \leq  \paren{\sumwts{t}} \left( 1-(1-e^{-\eta}) \distvec{t}\cdot\costvec{t} \right)
\]}
\item
for \R{$t=1,\ldots,T$} 
\item yields
\R{\begin{eqnarray*}
\sumwts{T+1} &\leq& N \prod_{t=1}^T (1-(1-e^{-\eta})
                         \distvec{t}\cdot\costvec{t}) \\
~\pause &\leq& \exp\paren{\ln N -(1-e^{-\eta}) \sum_{t=1}^T
                         \distvec{t}\cdot\costvec{t}} \\
\end{eqnarray*}}
since \R{$1+x\leq e^x$} for $x = -(1-e^{-\eta})$.
\end{itemize}
\end{frame}

\subsection{Lower bound on $\sumwts{\iter+1}$}

\begin{frame}
\frametitle{Lower bound on $\sumwts{\iter+1}$}

For any \R{$j=1,\ldots,N$}:
\R{\[
\sumwts{\iter+1} \geq \wt{\iter+1}{j} = e^{-\eta \lossi{j}}
\]}

\end{frame}

\subsection{Combining Upper and Lower bounds}

\begin{frame}
\frametitle{Combining Upper and Lower bounds}
\begin{itemize}
\item
Combining bounds on \R{$\ln \paren{\sumwts{\iter+1}}$}
\R{\[
 -\eta \lossi{j} \leq \ln \sumwts{\iter+1} 
 \leq \ln N -(1-e^{-\eta}) \sum_{t=1}^T \distvec{t}\cdot\costvec{t}
\]}
\item
Reversing signs, using \R{$\lossouralg = \sum_{t=1}^T \distvec{t}\cdot\costvec{t}$} 
and reorganizing we get
\R{\[
\lossouralg \leq \frac{\ln N + \eta \lossi{i}}
		      {1-e^{-\eta}}
\]}
\end{itemize}
\end{frame}

\section{tuning $\eta$}

\begin{frame}
\frametitle{Tuning \R{$\eta$}}
\includegraphics[height=7cm]{figures/beta-bounds.jpg}
\end{frame}

\begin{frame}
\frametitle{Tuning \R{$\eta$}}
\begin{itemize}
\item Suppose \R{$\min_i \lossi{i} \leq \upbnd{L}$}
\item set
\R{\[
\eta = \ln \paren{ 1+ \sqrt{\frac{2 \ln N}{\upbnd{L}}}} \approx \sqrt{\frac{2 \ln N}{\upbnd{L}}}
\]}
\item Then
\R{\[
\lossouralg \leq \frac{-\ln(\wt{1}{i}) + \eta \lossi{i}}
		      {1-e^{-\eta}}
\leq \min_i \lossi{i} + \sqrt{2 \upbnd{L} \ln N} + \ln N
\]}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Tuning \R{$\eta$} as a function of \R{$T$}}
\begin{itemize}
\item trivially \R{$\min_i \lossi{i} \leq T$}, yielding
\R{\[
\lossouralg \leq \min_i \lossi{i} + \sqrt{2 T \ln N} + \ln N
\]}
\item per iteration we get:
\R{\[
\frac{\lossouralg}{T} \leq \min_i \frac{\lossi{i}}{T} + \sqrt{\frac{2 \ln N}{T}} + \frac{\ln N}{T}
\]}
\end{itemize}
\end{frame}

\section{Lower Bounds}
\begin{frame}
\frametitle{How good is this bound?}
\begin{itemize}
\item
{\color{blue} Very good!} There is a closely matching lower bound!
\item
There exists a stochastic adversarial strategy such that with high
probability for \alert{any} hedging strategy \R{${\algfnt S}$} after \R{$T$} trials
\R{\[
\lossS - \min_i \lossi{i} \geq (1-o(1)) \sqrt{2T \ln N}
\]}
\item
The adversarial strategy is random, extremely simple, and does not
depend on the hedging strategy! 
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{The adversarial strategy}
\begin{itemize}
\item
Adversary sets each loss \R{$\cost{t}{i}$} indepedently at random \\
to \R{$0$} or \R{$1$} with equal probabilities $(1/2,1/2)$.
\item
Obviously, nothing to learn !\\
\R{$\lossS \approx T/2$.}
\item
On the other hand \R{$\min_i \lossi{i} \approx T/2 - \sqrt{2T \ln N}$}
\item
The difference \R{$\lossS - \min_i \lossi{i}$} is due to unlearnable
random fluctuations!
\item Detailed proof in ``Adaptive Game playing using multiplicative weights'' Freund and Schapire.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{The adversarial construction}
\includegraphics[height=7cm]{figures/randomWalk.pdf}
\end{frame}

\begin{frame}
\frametitle{Summary}
\begin{itemize}
\item Given learning rate \R{$\eta$} the \ouralg algorithm satisfies
\R{\[
\lossouralg \leq \frac{\ln N + \eta \lossi{i}}
		      {1-e^{-\eta}}
\]}
\item Setting \R{$\eta \approx \sqrt{\frac{2 \ln N}{T}}$} guarantees
\R{\[
\lossouralg \leq \min_i \lossi{i} + \sqrt{2 T \ln N} + \ln N
\]}
\item
A trivial random data, for which there is nothing to be learned forces
\alert{any} algorithm to suffer this total regret.
\end{itemize}
\end{frame}


\end{document}


