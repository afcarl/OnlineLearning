\documentclass{beamer}
%\documentclass[handout]{beamer}

\mode<presentation>
{
  \usetheme{Montpellier}

  %\setbeamercovered{transparent}
  % or whatever (possibly just delete it)
}

\usepackage{xmpmulti} % package that defines \multiinclude
\usepackage[english]{babel}
\usepackage[latin1]{inputenc}

\usepackage{times}
\usepackage[T1]{fontenc}

% Or whatever. Note that the encoding and the font should match. If T1
% does not look nice, try deleting the line with the fontenc.

\title[\ouralg] % (optional, use only with long paper titles)
{Estimation, Tracking and control using \ouralg}

\author[Freund] % (optional, use only with lots of authors)
{Yoav Freund}
% - Give the names in the same order as the appear in the paper.
% - Use the \inst{?} command only if the authors have different
%   affiliation.

\institute[Universities of Somewhere and Elsewhere] % (optional, but mostly needed)

\subject{Machine Learning}
% This is only inserted into the PDF information catalog. Can be left
% out. 

% If you have a file called "university-logo-filename.xxx", where xxx
% is a graphic format that can be processed by latex or pdflatex,
% resp., then you can add a logo as follows:

% \pgfdeclareimage[height=0.5cm]{university-logo}{university-logo-filename}
% \logo{\pgfuseimage{university-logo}}



% Delete this, if you do not want the table of contents to pop up at
% the beginning of each subsection:
%% \AtBeginSubsection[]
%% {
%%   \begin{frame}<beamer>
%%     \frametitle{Outline}
%%     \tableofcontents[currentsection,currentsubsection]
%%   \end{frame}
%% }


% If you wish to uncover everything in a step-wise fashion, uncomment
% the following command: 

\beamerdefaultoverlayspecification{<+->}

\input{macros}

\begin{document}

\begin{frame}
  \titlepage
\end{frame}

\begin{frame}
  \frametitle{Outline}
  \tableofcontents[pausesections]
  % You might wish to add the option [pausesections]
\end{frame}

\section{Estimation}

\begin{frame}
\frametitle{The static discrete estimation problem}
\begin{itemize}
\item A system has an internal unobservable \blue{state}:
\redEq{s \in \{0,1,\ldots,K\}}
\item \blue{observations} are corrupted versions of the state: 
\redEq{o_1,o_2,\dots\;\; o_t \in \{0,1,\ldots,K\}}
\item \blue{Goal}: given \redEq{o_1,o_2,\dots,o_t} compute prediction
\redEq{p_{t+1}} that is close to \redEq{o_{t+1}}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Generative modeling}
\begin{itemize}
\item Prob. of observation \redEq{o} conditioned on state \redEq{s}:\newline
\redEq{P\paren{O=o | S=s}}
\item Given \redEq{o_1,o_2,\dots,o_T} define likelihood of state
  \redEq{s} as \redEq{\prod_{t=1}^T P\paren{O=o_t | S=s}}.
\item max-likelihood estimator \redEq{\hat{s}}: \redEq{s} 
that maximized likelihood or log-likelihood:\newline
\redEq{\sum_{t=1}^T \log P\paren{O=o_t | S=s}}
\item Predict using the estimate: \redEq{p_{t+1}=\hat{s}}
\item Real-world problem - often \redEq{P\paren{O=o | S=s}} - the
  correct conditional distribution -  is not known!
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Estimation by minimizing loss}
\begin{itemize}
\item Define a loss function to measure the discrepency between the
  prediction \redEq{p_{t+1}} and observation \redEq{o_{t+1}}.
\item Examples: \redEq{\ell_1(p,o) = |p-o|}, 
\redEq{\ell_2(p,o) = (p-o)^2}
\item Given \redEq{o_1,o_2,\dots,o_T} use the minimal loss estimate.
\item No such thing as a \blue{``correct''} loss. 
\item No direct relation to estimation of hidden state.
\item Often useful to define an \blue{outlier insensitive loss}:
\redEq{\ell(p,o) = \min \paren{c,|p-o|}}
\item Leads to non-convex optimization problems.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Estimation using \ouralg}
\begin{itemize}
\item Consider each prediction \redEq{p=0,1,\ldots,K} as an expert.
\item \ouralg will define a distribution over \redEq{0,1,\ldots,K}.
\item The distribution defines our \blue{confidence interval}.
\item Guaranteed to perform almost as well as the best estimate in hindsite.
\item Similar to Bayesian posterior distribution, but does not assume
  known distribution of noise.
\end{itemize}
\end{frame}

\section{Tracking}

\begin{frame}
\frametitle{The discrete {\bf tracking} problem}
\begin{itemize}
\item The internal state of the system changes \blue{\bf slowly} over
  time. \newline
\redEq{s_1,s_2,\ldots\;\; s_t \in \{0,1,\ldots,K\}}
\item \blue{observations} are corrupted versions of the state: 
\redEq{o_1,o_2,\dots\;\; o_t \in \{0,1,\ldots,K\}}
\item \blue{Goal}: for each \redEq{t}, given \redEq{o_1,o_2,\dots,o_t}
compute \redEq{p_{t+1}} - a prediction of \redEq{o_{t+1}}.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Tracking using Generative models}
\begin{itemize}
\item To define a time varying generative model we need to know the
  {\bf correct} distribution of the state transitions
\item Hidden Markov Models: define \redEq{Pr(S_{t+1}|S_t)}
\item likelihood of observed sequence equal to sum over all possible
  hidden sequences, but can be computed efficiently using dynamic
  programming. 
\item Popular method in speech recognition.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Tracking by loss minimization}
\begin{itemize}
\item Only assumption: state changes \blue{slowly}.
\item Instead of cumulative loss, use exponentially discounted loss
\redEq{L_{t+1} = (1-\alpha) L_t + \ell_t}
\item \redEq{\alpha=0} corresponds to standard cumulative loss.
\item \redEq{\alpha>0} corresponds approximately to averaging over the
  previous \redEq{1/\alpha} iterations. 
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Using \ouralg for tracking}
\begin{itemize}
\item Use exponentially discounted loss.
\item \ouralg guaranteed to perform almost as well as best expert
  \blue{with respect to exp. discounted loss}.
\item Tracks state well when changes occur every \redEq{1/\alpha}
  examples.
\item Choosing the learning rate \redEq{\eta} is a significant problem.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Tracking using a noisy echo}
\multiinclude[graphics={width=4in},format=png]{figures/Noise}
\end{frame}

\begin{frame}
\frametitle{Tracking for \redEq{\sigma=1}}
\multiinclude[graphics={width=4in},format=png]{figures/sigma1}
\end{frame}

\begin{frame}
\frametitle{Tracking for \redEq{\sigma=2}}
\multiinclude[graphics={width=4in},format=png]{figures/sigma2}
\end{frame}

\begin{frame}
\frametitle{Tracking for \redEq{\sigma=4}}
\multiinclude[graphics={width=4in},format=png]{figures/sigma4}
\end{frame}

\begin{frame}
\frametitle{Tracking for \redEq{\sigma=8}}
\multiinclude[graphics={width=4in},format=png]{figures/sigma8}
\end{frame}

\begin{frame}
\frametitle{Tracking dynamical systems}
\begin{itemize}
\item Each expert can corresponds to a \blue{trajectory} in state space.
\item \blue{full state:} a description of the state that is sufficient
  to predict the future.
\item State of physical rigid object: location, speed, rotation,
  rotations speeds. In physics: \blue{``phase space''}
\item Without drift (dynamic noise) the trajectory is deterministically
  determined from the state.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Dealing with drift}
\begin{itemize}
\item \redEq{s_{t+1}} is not fully determined by \redEq{s_t}.
\item We borrow from generative modeling.
\item Split each expert \redEq{s_t} into a distribution over experts:
\redEq{Pr(S_{t+1}|S_t)}.
\item Use exponentially discounted loss to emphasize recent history.
\item Number of experts grows exponentially.
\item We can use dynamic programming - stil very expensive.
\item We use monte-carlo sampling.
\end{itemize}
\end{frame}

\section{Control}

\begin{frame}
\frametitle{The discrete {\bf control} problem}
\begin{itemize}
\item The internal state of the system (\blue{plant}) changes due to:
\begin{itemize}
\item Internal dynamics.
\item Extermal control signal.
\item Drift.
\end{itemize}
\item Goal of the controller given the observation history, generate a
  control signal to bring the plant close to desired state.
\begin{itemize}
\item In short amount of time.
\item Using small amount of power.
\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{control based on generative models}
\begin{itemize}
\item Requires generative model of plant
\begin{itemize}
\item Dynamics of plant.
\item Distribution model of drift.
\item Distribution of observation noise.
\end{itemize}
\item Analysis: combine controller and plant into a single dynamic
  system and analyze its properties under the generative model.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{control based on experts}
\begin{itemize}
\item Expert is a mapping from past observations to a control signal.
\item Loss is the difference between observation of desired
  observation (corresponding to desired state).
\item Similar to driving a car without understanding mechanics.
\item Experts \red{can} be based on generative models. \red{Can} estimate
  the state of the plant, etc.
\item Experts can be also be dumb! 
\item When system is complex, dumb expert is likely to perform better
  than complex expert.
\end{itemize}
\end{frame}

\end{document}


